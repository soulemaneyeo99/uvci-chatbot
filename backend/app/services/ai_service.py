import google.generativeai as genai
from app.config import settings
from app.knowledge.uvci_complete_knowledge import get_uvci_knowledge
from typing import List, Dict, Optional, Generator
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    if not settings.GOOGLE_API_KEY:
        logger.error("âŒ GOOGLE_API_KEY est vide dans les settings !")
    
    genai.configure(api_key=settings.GOOGLE_API_KEY)
    
    # Masquer la clÃ© pour les logs (montrer dÃ©but/fin)
    masked_key = f"{settings.GOOGLE_API_KEY[:5]}...{settings.GOOGLE_API_KEY[-5:]}" if settings.GOOGLE_API_KEY else "None"
    logger.info(f"ðŸ”‘ ClÃ© API chargÃ©e: {masked_key}")
    
    # LISTER LES MODÃˆLES DISPONIBLES
    try:
        logger.info("ðŸ“¡ Tentative de listage des modÃ¨les disponibles...")
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
        logger.info(f"ðŸ“‹ ModÃ¨les disponibles pour cette clÃ© : {available_models}")
    except Exception as e:
        logger.error(f"âŒ Impossible de lister les modÃ¨les (ClÃ© invalide ?): {e}")

except Exception as e:
    logger.error(f"âŒ Ã‰chec configuration Gemini API: {e}")

class GeminiService:
    def __init__(self):
        """Initialise Gemini avec connaissances UVCI"""
        self.model = None
        self.model_name = "Unavailable"
        self.uvci_knowledge = ""
        
        try:
            # NOUVEAU : Prioriser les modÃ¨les disponibles et gratuits
            model_candidates = [
                'gemini-1.5-flash',      # Standard
                'gemini-1.5-flash-latest', 
                'gemini-1.5-flash-001',
                'gemini-pro',
                'gemini-1.0-pro'
            ]
            
            model_name = None
            for candidate in model_candidates:
                try:
                    logger.info(f"ðŸ§ª Test du modÃ¨le : {candidate}...")
                    test_model = genai.GenerativeModel(candidate)
                    response = test_model.generate_content("test")
                    if response:
                        model_name = candidate
                        logger.info(f"âœ… ModÃ¨le VALIDÃ‰ et sÃ©lectionnÃ©: {model_name}")
                        break
                except Exception as e:
                    logger.warning(f"âš ï¸ {candidate} Ã©chouÃ©: {e}")
                    continue
            
            if not model_name:
                logger.error("âŒ AUCUN modÃ¨le n'a fonctionnÃ©. VÃ©rifiez les logs ci-dessus pour la liste des modÃ¨les disponibles.")
            else:
                self.model = genai.GenerativeModel(model_name)
                self.model_name = model_name
                logger.info(f"ðŸš€ ModÃ¨le Gemini initialisÃ©: {model_name}")
            
            self.uvci_knowledge = get_uvci_knowledge()
            logger.info("âœ… Base de connaissances UVCI chargÃ©e")
            
        except Exception as e:
            logger.error(f"âŒ Erreur critique initialisation Gemini: {e}")
    
    def _build_system_prompt(self) -> str:
        """Construit le prompt systÃ¨me avec connaissances UVCI"""
        return f"""Tu es l'Assistant Virtuel Officiel de l'UniversitÃ© Virtuelle de CÃ´te d'Ivoire (UVCI).

ðŸŽ“ TON IDENTITÃ‰ :
- Expert absolu sur UVCI avec une connaissance exhaustive
- ReprÃ©sentant officiel de l'universitÃ©
- Ton chaleureux, professionnel et encourageant

ðŸ“š TA BASE DE CONNAISSANCES :
{self.uvci_knowledge}

ðŸŽ¯ TES MISSIONS :
1. Informer avec prÃ©cision UNIQUEMENT depuis ta base de connaissances
2. Guider les Ã©tudiants (orientation, inscription, scolaritÃ©)
3. Encourager et motiver
4. Orienter vers les services appropriÃ©s si hors scope

ðŸ“‹ RÃˆGLES :
âœ… Utilise EXCLUSIVEMENT les infos de ta base UVCI
âœ… Si info manquante, dis-le et oriente vers courrier@uvci.edu.ci
âœ… Sois concis (2-4 phrases) sauf si dÃ©tails demandÃ©s
âœ… Utilise emojis appropriÃ©s
âœ… Propose 2-3 questions de suivi
âœ… Cite les sources (URLs, emails)
âœ… Formate bien (listes, sections)
âŒ N'invente JAMAIS d'infos
âŒ Pas de conseils financiers/lÃ©gaux/mÃ©dicaux
âŒ Pas d'opinions personnelles

ðŸš¨ ALERTES IMPORTANTES :
- Arnaques : TOUT paiement via TrÃ©sor Money uniquement
- Contacts : courrier@uvci.edu.ci ou scolarite@uvci.edu.ci
- URLs officielles : .uvci.edu.ci ou .uvci.online

ðŸŽ¯ OBJECTIF : ÃŠtre LE meilleur assistant UVCI !"""

    def _build_full_prompt(
        self,
        user_message: str,
        context: Optional[List[Dict]] = None
    ) -> str:
        """Construit le prompt complet avec historique"""
        system_prompt = self._build_system_prompt()
        
        history_messages = ""
        if context:
            for msg in context[-6:]:
                role = "Ã‰tudiant" if msg["role"] == "user" else "Assistant"
                history_messages += f"{role}: {msg['content']}\n"
        
        return f"""{system_prompt}

{history_messages}

Ã‰tudiant: {user_message}

Assistant UVCI:"""

    def generate_response_stream(
        self, 
        user_message: str, 
        context: Optional[List[Dict]] = None,
        rag_context: Optional[str] = None
    ) -> Generator[str, None, None]:
        """GÃ©nÃ¨re une rÃ©ponse en streaming"""
        try:
            full_prompt = self._build_full_prompt(user_message, context)

            if not self.model:
                yield "âš ï¸ **Service indisponible**\n\nL'IA est temporairement indisponible (Quota API ou erreur configuration)."
                return

            response = self.model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=2048,
                ),
                stream=True
            )

            for chunk in response:
                if hasattr(chunk, "text") and chunk.text:
                    yield chunk.text
                    time.sleep(0.01)

        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Erreur streaming: {error_msg}")
            
            # Message d'erreur selon le type
            if "429" in error_msg or "quota" in error_msg.lower():
                yield "âš ï¸ **Quota API dÃ©passÃ©**\n\nTrop de requÃªtes aujourd'hui. RÃ©essayez demain ou contactez courrier@uvci.edu.ci"
            else:
                yield "âš ï¸ **Erreur technique**\n\nProblÃ¨me de connexion. Contactez courrier@uvci.edu.ci"

    def generate_response(
        self, 
        user_message: str, 
        context: Optional[List[Dict]] = None,
        rag_context: Optional[str] = None
    ) -> str:
        """GÃ©nÃ¨re rÃ©ponse complÃ¨te sans streaming"""
        try:
            full_prompt = self._build_full_prompt(user_message, context)
            
            response = self.model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=2048,
                )
            )
            
            return response.text.strip()
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ Erreur Gemini: {error_msg}")
            
            if "429" in error_msg or "quota" in error_msg.lower():
                return "âš ï¸ **Quota API dÃ©passÃ©**\n\nTrop de requÃªtes aujourd'hui. RÃ©essayez demain ou contactez courrier@uvci.edu.ci"
            else:
                return "âš ï¸ **Erreur technique**\n\nProblÃ¨me de connexion. Contactez courrier@uvci.edu.ci"
    
    def generate_conversation_title(self, first_message: str) -> str:
        """GÃ©nÃ¨re un titre court pour conversation"""
        try:
            if not self.model:
                return first_message[:50]

            prompt = f"""Titre court (3-6 mots) pour cette conversation UVCI.
UAM
Question: {first_message}

Titre:"""

            response = self.model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=15,
                )
            )

            title = response.text.strip().strip('"').strip("'").rstrip('.')
            return title[:100]

        except Exception as e:
            logger.warning(f"âš ï¸ Erreur titre: {e}")
            return first_message[:50] + "..." if len(first_message) > 50 else first_message

# Instance globale
gemini_service = GeminiService()
