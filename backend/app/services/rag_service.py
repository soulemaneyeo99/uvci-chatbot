import chromadb
import google.generativeai as genai
from typing import List, Dict, Tuple
from app.config import settings
from app.utils.pdf_processor import pdf_processor
import os
import logging
import asyncio

logger = logging.getLogger(__name__)

class RAGService:
    """Service pour Retrieval Augmented Generation"""
    
    def __init__(self):
        # Configurer Gemini
        if settings.GOOGLE_API_KEY:
            genai.configure(api_key=settings.GOOGLE_API_KEY)
        else:
            logger.warning("‚ö†Ô∏è GOOGLE_API_KEY manquant. Le RAG ne fonctionnera pas.")

        # Cr√©er le dossier de persistance s'il n'existe pas
        persist_directory = "./data/chroma"
        os.makedirs(persist_directory, exist_ok=True)
        
        # Initialiser ChromaDB
        self.chroma_client = chromadb.PersistentClient(path=persist_directory)
        
        # Cr√©er ou r√©cup√©rer la collection
        try:
            self.collection = self.chroma_client.get_collection("uvci_documents")
            logger.info("‚úÖ Collection ChromaDB existante r√©cup√©r√©e")
        except:
            self.collection = self.chroma_client.create_collection(
                name="uvci_documents",
                metadata={"description": "Documents UVCI pour RAG"}
            )
            logger.info("‚úÖ Nouvelle collection ChromaDB cr√©√©e")
        
        logger.info("‚úÖ RAG Service initialis√© (Gemini Embeddings)")
    
    def _get_embedding(self, text: str) -> List[float]:
        """G√©n√®re un embedding avec Gemini"""
        try:
            # Nettoyer et tronquer si n√©cessaire (limite Gemini)
            if len(text) > 9000:
                text = text[:9000]
            
            # Utiliser le mod√®le d'embedding le plus r√©cent
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_document",
                title="Document chunk"
            )
            return result['embedding']
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur embedding text-embedding-004, essai embedding-001: {str(e)}")
            try:
                # Fallback sur l'ancien mod√®le
                result = genai.embed_content(
                    model="models/embedding-001",
                    content=text,
                    task_type="retrieval_document",
                    title="Document chunk"
                )
                return result['embedding']
            except Exception as e2:
                logger.error(f"‚ùå Erreur embedding persistante: {str(e2)}")
                return []

    def _get_query_embedding(self, text: str) -> List[float]:
        """G√©n√®re un embedding pour une requ√™te"""
        try:
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=text,
                task_type="retrieval_query"
            )
            return result['embedding']
        except Exception as e:
            try:
                result = genai.embed_content(
                    model="models/embedding-001",
                    content=text,
                    task_type="retrieval_query"
                )
                return result['embedding']
            except Exception as e2:
                logger.error(f"‚ùå Erreur embedding requ√™te Gemini: {str(e2)}")
                return []

    def index_document(self, document_id: str, file_path: str, filename: str) -> int:
        """
        Index un document PDF dans la base vectorielle
        """
        try:
            # 1. Extraire le texte du PDF
            logger.info(f"üìÑ Extraction du texte de {filename}...")
            raw_text = pdf_processor.extract_text(file_path)
            
            if not raw_text or len(raw_text) < 100:
                logger.warning(f"‚ö†Ô∏è  Texte trop court ou vide pour {filename}")
                return 0
            
            # 2. Nettoyer le texte
            clean_text = pdf_processor.clean_text(raw_text)
            
            # 3. D√©couper en chunks
            # Augmenter la taille des chunks pour Gemini (il g√®re mieux le contexte)
            chunks = pdf_processor.chunk_text(
                clean_text,
                chunk_size=1000, 
                overlap=200
            )
            
            logger.info(f"‚úÇÔ∏è  {len(chunks)} chunks cr√©√©s pour {filename}")
            
            # 4. Cr√©er les embeddings et ajouter √† ChromaDB
            ids = []
            embeddings = []
            valid_chunks = []
            metadatas = []

            for i, chunk in enumerate(chunks):
                # Pause pour √©viter de spammer l'API (Rate limit)
                # Note: Sur la version synchrone on ne peut pas await, mais c'est rapide.
                
                embedding = self._get_embedding(chunk)
                if embedding:
                    ids.append(f"{document_id}_chunk_{i}")
                    embeddings.append(embedding)
                    valid_chunks.append(chunk)
                    metadatas.append({
                        "document_id": document_id,
                        "filename": filename,
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    })
            
            if not ids:
                logger.warning("Aucun embedding g√©n√©r√©.")
                return 0

            # 5. Ajouter √† ChromaDB par lots (batch)
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=valid_chunks,
                metadatas=metadatas
            )
            
            logger.info(f"‚úÖ {len(ids)} chunks index√©s avec succ√®s")
            return len(ids)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'indexation: {str(e)}")
            return 0
    
    def search(self, query: str, top_k: int = None) -> Tuple[List[str], List[str]]:
        """
        Recherche les chunks pertinents pour une requ√™te
        """
        if top_k is None:
            top_k = settings.TOP_K_RESULTS
        
        try:
            # 1. Cr√©er l'embedding de la requ√™te
            query_embedding = self._get_query_embedding(query)
            
            if not query_embedding:
                return [], []

            # 2. Rechercher dans ChromaDB
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            
            if not results['documents'] or not results['documents'][0]:
                return [], []
            
            # 3. Extraire les chunks et sources
            chunks = results['documents'][0]
            sources = [meta['filename'] for meta in results['metadatas'][0]]
            
            return chunks, sources
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche RAG: {str(e)}")
            return [], []
    
    def get_rag_context(self, query: str) -> Tuple[str, List[str]]:
        """
        R√©cup√®re le contexte RAG format√© pour Gemini
        
        Returns:
            (context_text, sources) - Contexte format√© et liste des sources
        """
        chunks, sources = self.search(query)
        
        if not chunks:
            return "", []
        
        # Formater le contexte
        context_parts = []
        unique_sources = list(set(sources))
        
        for i, chunk in enumerate(chunks):
            source = sources[i]
            context_parts.append(f"[Document: {source}]\n{chunk}\n")
        
        context_text = "\n---\n".join(context_parts)
        return context_text, unique_sources
    
    def list_documents(self) -> List[Dict]:
        """Liste tous les documents index√©s (uniques)"""
        try:
            results = self.collection.get(include=['metadatas'])
            metadatas = results['metadatas']
            
            docs_map = {}
            for meta in metadatas:
                if meta and 'document_id' in meta:
                    doc_id = meta['document_id']
                    if doc_id not in docs_map:
                        docs_map[doc_id] = {
                            "id": doc_id,
                            "filename": meta.get('filename', 'Inconnu'),
                            "chunk_count": meta.get('total_chunks', 0),
                            "upload_date": meta.get('upload_date', None)
                        }
            
            return list(docs_map.values())
        except Exception as e:
            logger.error(f"‚ùå Erreur list_documents: {str(e)}")
            return []

    def delete_document_chunks(self, document_id: str):
        """Supprime tous les chunks d'un document"""
        try:
            # R√©cup√©rer tous les IDs des chunks du document
            results = self.collection.get(
                where={"document_id": document_id}
            )
            
            if results['ids']:
                self.collection.delete(ids=results['ids'])
                logger.info(f"üóëÔ∏è  {len(results['ids'])} chunks supprim√©s")
                return True
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur suppression chunks: {str(e)}")
            return False

# Instance globale
rag_service = RAGService()